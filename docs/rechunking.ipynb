{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechunking lets us re-distribute how datasets are split between variables and chunks across a Beam PCollection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started we'll recreate our dummy data from the data model tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-inputs"
    ]
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import numpy as np\n",
    "import xarray_beam as xbeam\n",
    "import xarray\n",
    "\n",
    "def create_records():\n",
    "    for offset in [0, 4]:\n",
    "        key = xbeam.Key({'x': offset, 'y': 0})\n",
    "        data = 2 * offset + np.arange(8).reshape(4, 2)\n",
    "        chunk = xarray.Dataset({\n",
    "            'foo': (('x', 'y'), data),\n",
    "            'bar': (('x', 'y'), 100 + data),\n",
    "        })\n",
    "        yield key, chunk\n",
    "        \n",
    "inputs = list(create_records())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing chunks\n",
    "\n",
    "Chunking can be essential for some operations. Some operations are very hard or impossible to perform with certain chunking schemes. For example, to make a plot all the data needs to come toether on a single machine. Other calculations such as calculating a median are _possible_ to perform on distributed data, but require tricky algorithms and/or approximation.\n",
    "\n",
    "More broadly, chunking can have critical performance implications, similar to [those for Xarray and Dask](http://xarray.pydata.org/en/stable/user-guide/dask.html#chunking-and-performance). As a rule of thumb, chunk sizes of 10-100 MB work well. The optimal chunk size is a balance among a number of considerations, adapted here [from Dask docs](https://docs.dask.org/en/latest/array-chunks.html):\n",
    "\n",
    "1. Chunks should be small enough to fit comfortably into memory on a single machine. As an upper limit, chunks over roughly 2 GB in size will not fit into the protocol buffers Beam uses to pass data between workers. \n",
    "2. There should be enough chunks for Beam runners (like Cloud Dataflow) to elastically shard work over many workers.\n",
    "3. Chunks should be large enough to amortize the overhead of networking and the Python interpreter, which starts to become noticeable for arrays with fewer than 1 million elements.\n",
    "\n",
    "The `nbytes` attribute on both NumPy arrays and `xarray.Dataset` objects is a good easy way to figure out how larger chunks are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest transformation is splitting (or consoldating) different _variables_ in a Dataset with {py:class}`~xarray_beam.SplitVariables()` and {py:class}`~xarray_beam.ConsolidateVariables()`, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars={'foo'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7),\n",
       " (Key(offsets={'x': 0, 'y': 0}, vars={'bar'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107),\n",
       " (Key(offsets={'x': 4, 'y': 0}, vars={'foo'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 8 9 10 11 12 13 14 15),\n",
       " (Key(offsets={'x': 4, 'y': 0}, vars={'bar'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      bar      (x, y) int64 108 109 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitVariables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Instead of a separate transform for splitting variables, you can also set `split_vars=True` in {py:class}`~xarray_beam.DatasetToChunks`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also adjust _chunks_ in a dataset to distribute arrays of different sizes. Here you have two choices of API:\n",
    "\n",
    "1. The lower level {py:class}`~xarray_beam.SplitChunks` and {py:class}`~xarray_beam.ConsolidateChunks`. These transformations apply a single splitting (with indexing) or consolidation (with {py:function}`xarray.concat`) function to array elements.\n",
    "2. The high level {py:class}`~xarray_beam.Rechunk`, which uses a pipeline of multiple split/consolidate steps (as needed) to efficiently rechunk a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level rechunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For minor adjustments (e.g., mostly along a single dimension), the more explicit `SplitChunks()` and `ConsolidateChunks()` are good options. They take a dict of _desired_ chunk sizes as a parameter, which can also be `-1` to indicate \"no chunking\" along a dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 8, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 ... 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.ConsolidateChunks({'x': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because these transformations only split _or_ consolidate, they cannot necessary fully rechunk a dataset in a single step if the new chunk sizes are not multiples of old chunks (with consolidate) or do not even divide the old chunks (with split), e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107),\n",
       " (Key(offsets={'x': 4, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 1, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 8 9\n",
       "      bar      (x, y) int64 108 109),\n",
       " (Key(offsets={'x': 5, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitChunks({'x': 5})  # notice that the first two chunks are still separate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such uneven cases, you'll need to use split followed by consolidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 5, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107 108 109),\n",
       " (Key(offsets={'x': 5, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitChunks({'x': 5}) | xbeam.ConsolidateChunks({'x': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level rechunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the high-level `Rechunk()` method applies multiple split and consolidate steps based on the [Rechunker](https://github.com/pangeo-data/rechunker) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage0/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage0/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage2/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage2/Consolidate/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 5, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107 108 109),\n",
       " (Key(offsets={'x': 5, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.Rechunk(dim_sizes={'x': 6}, source_chunks={'x': 3}, target_chunks={'x': 5}, itemsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rechunk` requires specifying a few more parameters, but based on that information **it can be _much_ more efficient for more complex rechunking tasks**, particular in cases where data needs to be distributed into a very different shape (e.g., distributing a matrix across rows vs. columns).\n",
    "\n",
    "The naive \"splitting\" approach in such cases may divide datasets into extremely small tasks corresponding to individual array elements, which adds a huge amount of overhead."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "aef148d7ea0dbd1f91630322dd5bc9e24a2135d95f24fe1a9dab9696856be2b9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
