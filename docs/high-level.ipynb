{
  "cells": [
    {
      "metadata": {
        "id": "ZQCqHh_IxEf6"
      },
      "cell_type": "markdown",
      "source": [
        "# High-level datasets"
      ]
    },
    {
      "metadata": {
        "id": "z8MqtGzHxL8F"
      },
      "cell_type": "markdown",
      "source": [
        "{py:class}`xarray_beam.Dataset` is new (as of September 2025) high-level interface for Xarray-Beam.\n",
        "\n",
        "It requires less boilerplate code than the current (explicit) interface, and accordingly should be an easier to use tool, especially for non-expert users. You still need to think about how your data is divided into chunks, but the data model of `Dataset` keep track of the high-level structure of your data, avoiding the need to manually building templates in {py:class}`~xarray_beam.ChunksToZarr`.\n",
        "\n",
        "```{warning}\n",
        "The `Dataset` interface is experimental, and currently offers no backwards compatibility guarantees.\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "Ht5100QIRpyd",
        "tags": [
          "hide-cell"
        ]
      },
      "cell_type": "code",
      "source": [
        "# small formatting improvements\n",
        "import contextlib\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def print_error():\n",
        "  try:\n",
        "    yield\n",
        "  except Exception as e:\n",
        "    print(f'{type(e).__name__}: {e}')"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "EkH9Na4ezuTi"
      },
      "cell_type": "markdown",
      "source": [
        "## Data model\n",
        "\n",
        "Dataset is a wrapper over a series of Beam transformations, adding metadata describing the corresponding `xarray.Dataset` and how is distributed with Beam:\n",
        "\n",
        "- `ptransform` is the wrapped `beam.PTransform` to compute the chunks of the dataset.\n",
        "- `template` is a lazily-computed `xarray.Dataset` indicating the structure of the overall dataset.\n",
        "- `chunks` is dictionary mapping from dimension names to integer chunk sizes, indicating the size of each chunk.\n",
        "- `split_vars` is a boolean indicating if ptransform elements each contain only a single variable from the dataset, rather than all variables.\n",
        "\n",
        "This information is surfaced via `xbeam.Dataset.__repr__()`:"
      ]
    },
    {
      "metadata": {
        "id": "3N0wXjXRy9cK"
      },
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import xarray_beam as xbeam\n",
        "import xarray\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "xarray_ds = xarray.Dataset(\n",
        "    {'temperature': (('time', 'longitude', 'latitude'), np.random.randn(365, 180, 90))},\n",
        "    coords={'time': pd.date_range('2025-01-01', freq='1D', periods=365)},\n",
        ")\n",
        "chunks = {'time': 100, 'longitude': 90, 'latitude': 90}\n",
        "xbeam_ds = xbeam.Dataset.from_xarray(xarray_ds, chunks)\n",
        "xbeam_ds"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "k5U2hsNr51FJ"
      },
      "cell_type": "markdown",
      "source": [
        "Xarray-Beam pipelines typically read and write data to [Zarr](https://zarr.dev/), so we'll start by writing our example data to a Zarr file (with plain Xarray/Dask) for us to read later:"
      ]
    },
    {
      "metadata": {
        "id": "cqDLwyBG51FJ"
      },
      "cell_type": "code",
      "source": [
        "xarray_ds.chunk(chunks).to_zarr('example_data.zarr', mode='w')"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "fb-Wx7dl1ScK"
      },
      "cell_type": "markdown",
      "source": [
        "## Writing pipelines\n",
        "\n",
        "Most Xarray-Beam pipelines can be written via a handful of Dataset methods:\n",
        "\n",
        "- {py:meth}`~xarray_beam.Dataset.from_zarr`: Load a dataset from a Zarr store.\n",
        "- {py:meth}`~xarray_beam.Dataset.rechunk`: Adjust chunks on a dataset.\n",
        "- {py:meth}`~xarray_beam.Dataset.map_blocks`: Map a function over every chunk of this dataset independently.\n",
        "- {py:meth}`~xarray_beam.Dataset.to_zarr`: Write a dataset to a Zarr store.\n",
        "\n",
        "All non-trivial computation happens via the embarrasingly parallel `map_blocks` method.\n",
        "\n",
        "In order for `map_blocks` to work, data needs to be appropriately chunked. Here are a few typical chunking patterns that work well for most needs:\n",
        "\n",
        "- \"Pencil\" chunks, which group together all times, and parallelize over space. These long and skinny chunks look like a box of pencils:"
      ]
    },
    {
      "metadata": {
        "id": "EpOpwzZS9Rte"
      },
      "cell_type": "code",
      "source": [
        "xarray_ds.temperature.chunk({'time': -1, 'latitude': 20, 'longitude': 20}).data"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "74t0dh3E9kny"
      },
      "cell_type": "markdown",
      "source": [
        "- \"Pancake\" chunks, which group together all spatial locations, and parallelize over time. These flat and wide chunks look like a stack of pancakes:"
      ]
    },
    {
      "metadata": {
        "id": "fho4ub-69mD5"
      },
      "cell_type": "code",
      "source": [
        "xarray_ds.temperature.chunk({'time': 1, 'latitude': -1, 'longitude': -1}).data"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "t3APp6uB9yjT"
      },
      "cell_type": "markdown",
      "source": [
        "Weather/climate datasets are typically generated and stored in pancake chunks, but pencil chunks are more useful for most analytics queries, which requires large histories of weather at a single location. Intermediate \"compromise\" chunks can sometimes be a good idea, although if performance and flexibility are critical it may be worth storing multiple copies of your data in different formats.\n",
        "\n",
        "Using the right chunks is *absolutely essentially* for efficient operations with Xarray-Beam and Zarr. For example, reading data from a single location across all times (a \"pencil\" query) is extremely inefficient for a dataset stored in \"pancake\" chunks -- it would require loading the entire dataset from disk!\n",
        "\n",
        "Rechunking is a fundamentally an expensive operation (it requires multiple complete reads/writes of a dataset from disk), but in Xarray-Beam it's straightforward, via {py:meth}`~xarray_beam.Dataset.rechunk`."
      ]
    },
    {
      "metadata": {
        "id": "-SXovp9B6LwH"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 1: Climatology\n",
        "\n",
        "Here we need to group together all time points in the same chunk (\"pencil chunks\"), parallelizing over space:"
      ]
    },
    {
      "metadata": {
        "id": "WIOFiC1z40mh"
      },
      "cell_type": "code",
      "source": [
        "with beam.Pipeline() as p:\n",
        "  p | (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .rechunk({'time': -1, 'latitude': 30, 'longitude': 30})\n",
        "      .map_blocks(lambda ds: ds.groupby('time.month').mean())\n",
        "      .to_zarr('example_climatology.zarr')\n",
        "  )\n",
        "xarray.open_zarr('example_climatology.zarr')"
      ],
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "O14EmS3J8e6H"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 2: Regridding over space\n",
        "\n",
        "Here we need to group all space points in the same chunk (\"pancake chunks\"), parallelizing over time:"
      ]
    },
    {
      "metadata": {
        "id": "kms-N92ZzWxy"
      },
      "cell_type": "code",
      "source": [
        "with beam.Pipeline() as p:\n",
        "  p | (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .rechunk({'time': 10, 'latitude': -1, 'longitude': -1})\n",
        "      .map_blocks(lambda ds: ds.coarsen(latitude=2, longitude=2).mean())\n",
        "      .to_zarr('example_regrid.zarr')\n",
        "  )\n",
        "xarray.open_zarr('example_regrid.zarr')"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "FpJBZ6XsB_oE"
      },
      "cell_type": "markdown",
      "source": [
        "## Limitations of map_blocks\n",
        "\n",
        "In the examples above, {py:meth}`~xarray_beam.Dataset.map_blocks` somehow automatically knew the appropriate structure of the output `template`, without evaluating any chunked data. How could this work?\n",
        "\n",
        "For building templates, Xarray-Beam relies on lazy evaluation with [Dask arrays](https://docs.dask.org/en/stable/array.html). This requires that applied functions are Dask compatible. Almost all built-in Xarray operations are Dask compatible, but if your applied function is _not_ Dask compatible (e.g., because it loads array values into memory), Xarray-Beam will show an informative error:"
      ]
    },
    {
      "metadata": {
        "id": "4TB_pzjTDLfS"
      },
      "cell_type": "code",
      "source": [
        "with print_error():\n",
        "  (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .map_blocks(lambda ds: ds.compute())  # load into memory\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "vCjZK9fmEeEq"
      },
      "cell_type": "markdown",
      "source": [
        "You can avoid these errors by explicitly [creating a template](creating_templates):"
      ]
    },
    {
      "metadata": {
        "id": "yOGT81UYC2-0"
      },
      "cell_type": "code",
      "source": [
        "ds_beam = xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "ds_beam.map_blocks(lambda ds: ds.compute(), template=ds_beam.template)"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "75IG-22cKcuE"
      },
      "cell_type": "markdown",
      "source": [
        "In other situations, you might want to perform an operation that returns something other than an `xarray.Dataset`, e.g., to write all chunks as individual files to disk. In these situations, you can switch to the lower-level Xarray-Beam [data model](data-model), and use raw Beam operations:"
      ]
    },
    {
      "metadata": {
        "id": "SlnFfo2YKZUN"
      },
      "cell_type": "code",
      "source": [
        "def to_netcdf(key: xbeam.Key, chunk: xarray.Dataset):\n",
        "  path = f\"{chunk.indexes['time'][0]:%Y-%m-%d}.nc\"\n",
        "  chunk.to_netcdf(path)\n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "  p | (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .rechunk({'latitude': -1, 'longitude': -1})\n",
        "      .ptransform\n",
        "  ) | beam.MapTuple(to_netcdf)\n",
        "\n",
        "%ls *.nc"
      ],
      "outputs": [],
      "execution_count": 14
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
