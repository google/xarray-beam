{
  "cells": [
    {
      "metadata": {
        "id": "ZQCqHh_IxEf6"
      },
      "cell_type": "markdown",
      "source": [
        "# High-level datasets"
      ]
    },
    {
      "metadata": {
        "id": "z8MqtGzHxL8F"
      },
      "cell_type": "markdown",
      "source": [
        "{py:class}`xarray_beam.Dataset` is new (as of September 2025) high-level interface for Xarray-Beam.\n",
        "\n",
        "It requires less boilerplate code than the current (explicit) interface, and accordingly should be an easier to use tool, especially for non-expert users. You still need to think about how your data is divided into chunks, but the data model of `Dataset` keep track of the high-level structure of your data, avoiding the need to manually building templates in {py:class}`~xarray_beam.ChunksToZarr`.\n",
        "\n",
        "```{warning}\n",
        "The `Dataset` interface is experimental, and currently offers no backwards compatibility guarantees.\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "EkH9Na4ezuTi"
      },
      "cell_type": "markdown",
      "source": [
        "## Data model\n",
        "\n",
        "Dataset is a wrapper over a series of Beam transformations, adding metadata describing the corresponding `xarray.Dataset` and how is distributed with Beam:\n",
        "\n",
        "- `ptransform` is the wrapped `beam.PTransform` to compute the chunks of the dataset.\n",
        "- `template` is a lazily-computed `xarray.Dataset` indicating the structure of the overall dataset.\n",
        "- `chunks` is dictionary mapping from dimension names to integer chunk sizes, indicating the size of each chunk.\n",
        "- `split_vars` is a boolean indicating if ptransform elements each contain only a single variable from the dataset, rather than all variables.\n",
        "\n",
        "This information is surfaced via `xbeam.Dataset.__repr__()`:"
      ]
    },
    {
      "metadata": {
        "id": "3N0wXjXRy9cK"
      },
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import xarray_beam as xbeam\n",
        "import xarray\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "xarray_ds = xarray.Dataset(\n",
        "    {'temperature': (('time', 'longitude', 'latitude'), np.random.randn(365, 360, 180))},\n",
        "    coords={'time': pd.date_range('2025-01-01', freq='1D', periods=365)},\n",
        ")\n",
        "chunks = {'time': '30MB', 'longitude': -1, 'latitude': -1}\n",
        "xbeam_ds = xbeam.Dataset.from_xarray(xarray_ds, chunks)\n",
        "xbeam_ds"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "k5U2hsNr51FJ"
      },
      "cell_type": "markdown",
      "source": [
        "Xarray-Beam pipelines typically read and write data to [Zarr](https://zarr.dev/), so we'll start by writing our example data to a Zarr file (with plain Xarray/Dask) for us to read later:"
      ]
    },
    {
      "metadata": {
        "id": "cqDLwyBG51FJ"
      },
      "cell_type": "code",
      "source": [
        "xarray_ds.chunk(chunks).to_zarr('example_data.zarr', mode='w')"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "fb-Wx7dl1ScK"
      },
      "cell_type": "markdown",
      "source": [
        "## Writing pipelines\n",
        "\n",
        "Most Xarray-Beam pipelines can be written via a handful of Dataset methods:\n",
        "\n",
        "- {py:meth}`~xarray_beam.Dataset.from_zarr`: Load a dataset from a Zarr store.\n",
        "- {py:meth}`~xarray_beam.Dataset.rechunk`: Adjust chunks on a dataset.\n",
        "- {py:meth}`~xarray_beam.Dataset.map_blocks`: Map a function over every chunk of this dataset independently.\n",
        "- {py:meth}`~xarray_beam.Dataset.to_zarr`: Write a dataset to a Zarr store.\n",
        "\n",
        "All non-trivial computation happens via the embarrasingly parallel `map_blocks` method.\n",
        "\n",
        "### Chunking strategies\n",
        "\n",
        "In order for `map_blocks` to work, data needs to be appropriately chunked. Here are a few typical chunking patterns that work well for most needs:\n",
        "\n",
        "- \"Pencil\" chunks, which group together all times, and parallelize over space. These long and skinny chunks look like a box of pencils.\n",
        "- \"Pancake\" chunks, which group together all spatial locations, and parallelize over time. These flat and wide chunks look like a stack of pancakes.\n",
        "- Intermediate \"compromise\" chunks, somewhere between these extremes. These optimize for worst-case behavior. "
      ]
    },
    {
      "metadata": {
        "id": "e7dkTphQyLzz"
      },
      "cell_type": "markdown",
      "source": [
        "![Pancake vs Pencil chunks](./_static/pancake-vs-pencil.svg)\n"
      ]
    },
    {
      "metadata": {
        "id": "t3APp6uB9yjT"
      },
      "cell_type": "markdown",
      "source": [
        "Weather/climate datasets are typically generated and stored in \"pancake\" chunks, but \"pencil\" chunks are more suitable for most analytics queries, which requires large histories of weather at small numbers of locations.\n",
        "\n",
        "Using the right chunks is *absolutely essentially* for efficient operations with Xarray-Beam and Zarr. For example, reading data from a single location across all times (a \"pencil\" query) is extremely inefficient for a dataset stored in \"pancake\" chunks -- it would require loading the entire dataset from disk!\n",
        "\n",
        "[Like Dask](https://docs.dask.org/en/latest/array-best-practices.html), Xarray-Beam works best with chunks that are 10s to 100s of MB in size, large enough to amortize Python overhead but small enough that chunks fit easily into memory. Smaller chunk sizes in Zarr stores (closer to 10MB) can be convenient to increase the flexiblity of chunking methods when reading the data from disk later\n",
        "\n",
        "You can explicitly adjusts chunk sizes with {py:meth}`~xarray_beam.Dataset.rechunk`. The syntax is a mapping from dimension names (or `...`, for all other dimensions) to chunk sizes, which can be any of `-1` (to indicate \"size of the full dimension\"), a positive integer or a string indicating a target size in bytes like `'100MB'`, e.g.,\n",
        "- `chunks=-1`: no chunking.\n",
        "- `chunks='100 MB'`: chunk size of ~100 MB, without dimension-specific constraints.\n",
        "- `chunks={'time': 100}`: fixed size of 100 along time, preserving original chunks along other dimensions.\n",
        "- `chunks={'time': -1, ...: '100 MB'}`: pencil chunks of ~100 MB each.\n",
        "- `chunks={'latitude': -1, 'longitude': -1, ...: '100 MB'}`: pancake chunks of ~100 MB each.\n",
        "\n",
        "See {py:func}`~xarray_beam.normalize_chunks` for full details."
      ]
    },
    {
      "metadata": {
        "id": "noNiRxGrGX3L"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "```{warning}\n",
        "Rechunking between very different schemes is fundamentally an expensive operation (it requires multiple complete reads/writes of a dataset from disk). If performance and flexibility are critical, it may be worth storing multiple copies of your data in different chunking formats.\n",
        "```\n",
        "\n",
        "```{tip}\n",
        "Using Zarr v3's sharding feature in {py:meth}`~xarray_beam.Dataset.to_zarr` to group smaller chunks (~1 MB) into shards can also help mitigate the challenges of picking an optimal chunk size.\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "-SXovp9B6LwH"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 1: Climatology\n",
        "\n",
        "Here we need to group together all time points in the same chunk (\"pencil chunks\"), parallelizing over space:"
      ]
    },
    {
      "metadata": {
        "id": "WIOFiC1z40mh"
      },
      "cell_type": "code",
      "source": [
        "with beam.Pipeline() as p:\n",
        "  p | (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .rechunk({'time': -1, ...: '100 MB'})\n",
        "      .map_blocks(lambda ds: ds.groupby('time.month').mean())\n",
        "      .rechunk('10 MB')  # ensure a reasonable min chunk-size for Zarr\n",
        "      .to_zarr('example_climatology.zarr')\n",
        "  )\n",
        "xarray.open_zarr('example_climatology.zarr')"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "O14EmS3J8e6H"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 2: Regridding over space\n",
        "\n",
        "Here we need to group all space points in the same chunk (\"pancake chunks\"), parallelizing over time:"
      ]
    },
    {
      "metadata": {
        "id": "kms-N92ZzWxy"
      },
      "cell_type": "code",
      "source": [
        "with beam.Pipeline() as p:\n",
        "  p | (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .rechunk({'time': '30MB', 'latitude': -1, 'longitude': -1})\n",
        "      .map_blocks(lambda ds: ds.coarsen(latitude=2, longitude=2).mean())\n",
        "      .to_zarr('example_regrid.zarr')\n",
        "  )\n",
        "xarray.open_zarr('example_regrid.zarr')"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "FpJBZ6XsB_oE"
      },
      "cell_type": "markdown",
      "source": [
        "## Limitations of map_blocks\n",
        "\n",
        "In the examples above, {py:meth}`~xarray_beam.Dataset.map_blocks` somehow automatically knew the appropriate structure of the output `template`, without evaluating any chunked data. How could this work?\n",
        "\n",
        "For building templates, Xarray-Beam relies on lazy evaluation with [Dask arrays](https://docs.dask.org/en/stable/array.html). This requires that applied functions are Dask compatible. Almost all built-in Xarray operations are Dask compatible, but if your applied function is _not_ Dask compatible (e.g., because it loads array values into memory), Xarray-Beam will show an informative error:"
      ]
    },
    {
      "metadata": {
        "id": "4TB_pzjTDLfS"
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  (\n",
        "      xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "      .map_blocks(lambda ds: ds.compute())  # load into memory\n",
        "  )\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}: {e}')"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "vCjZK9fmEeEq"
      },
      "cell_type": "markdown",
      "source": [
        "You can avoid these errors by explicitly [creating a template](creating_templates):"
      ]
    },
    {
      "metadata": {
        "id": "yOGT81UYC2-0"
      },
      "cell_type": "code",
      "source": [
        "(\n",
        "    xbeam.Dataset.from_zarr('example_data.zarr')\n",
        "    .map_blocks(lambda ds: ds.compute(), template=ds_beam.template)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "-U4t0kKIkDvb"
      },
      "cell_type": "markdown",
      "source": [
        "## Interfacing with low-level transforms"
      ]
    },
    {
      "metadata": {
        "id": "75IG-22cKcuE"
      },
      "cell_type": "markdown",
      "source": [
        "`Dataset` is a thin wrapper around Xarray-Beam transformations, so you can always drop into the lower-level Xarray-Beam [data model](data-model) and use raw Beam operations. This is especially useful for the reading or writing data.\n",
        "\n",
        "```{warning}\n",
        "The `Dataset` constructor currently performs **no validation** on its inputs!\n",
        "```\n",
        "\n",
        "For example, here's how you could manually recreate a `Dataset`, using the common pattern of evaluating a single example in-memory to create a template with {py:func}`~xarray_beam.make_template` and {py:func}`~xarray_beam.replace_template_dims`:"
      ]
    },
    {
      "metadata": {
        "id": "l9pHS1QDlMd-"
      },
      "cell_type": "code",
      "source": [
        "all_times = pd.date_range('2025-01-01', freq='1D', periods=365)\n",
        "source_dataset = xarray.open_zarr('example_data.zarr', chunks=None)\n",
        "\n",
        "def load_chunk(time: pd.Timestamp) -\u003e tuple[xbeam.Key, xarray.Dataset]:\n",
        "  key = xbeam.Key({'time': (time - all_times[0]).days})\n",
        "  dataset = source_dataset.sel(time=[time])\n",
        "  return key, dataset\n",
        "\n",
        "_, example = load_chunk(all_times[0])\n",
        "\n",
        "template = xbeam.make_template(example)\n",
        "template = xbeam.replace_template_dims(template, time=all_times)\n",
        "\n",
        "ds_beam = xbeam.Dataset(\n",
        "    template=template,\n",
        "    chunks=xbeam.normalize_chunks({'time': 1}, template),\n",
        "    split_vars=False,\n",
        "    ptransform=(beam.Create(all_times) | beam.Map(load_chunk)),\n",
        ")\n",
        "ds_beam"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "1qjeY5mwlLGJ"
      },
      "cell_type": "markdown",
      "source": [
        "You can also pull-out the underlying Beam `ptransform` from a dataset to append new transformations, e.g., to write each element of the pipeline to disk as a separate file:"
      ]
    },
    {
      "metadata": {
        "id": "SlnFfo2YKZUN"
      },
      "cell_type": "code",
      "source": [
        "def to_netcdf(key: xbeam.Key, chunk: xarray.Dataset):\n",
        "  path = f\"{chunk.indexes['time'][0]:%Y-%m-%d}.nc\"\n",
        "  chunk.to_netcdf(path)\n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "  p | ds_beam.rechunk('50MB').ptransform | beam.MapTuple(to_netcdf)\n",
        "\n",
        "%ls *.nc"
      ],
      "outputs": [],
      "execution_count": 13
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
