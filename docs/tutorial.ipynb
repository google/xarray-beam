{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Xarray-Beam is a library for writing [Apache Beam](http://beam.apache.org/) pipelines consisting of [xarray](http://xarray.pydata.org) Dataset objects. This tutorial (and Xarray-Beam itself) assumes basic familiarity with both Beam and Xarray.\n",
    "\n",
    "This tutorial will walk you through the basics of writing a pipeline with Xarray-Beam. We also recommend reading through a few [end to end examples](https://github.com/google/xarray-beam/tree/main/examples) to understand what code using Xarray-Beam typically looks like.\n",
    "\n",
    "```{note}\n",
    "Before getting started, it's important to understand that although Xarray-Beam tries to make it _straightforward_ to write distributed pipelines with Xarray objects, but it doesn't try to hide the distributed magic inside high-level objects like [Xarray with Dask](http://xarray.pydata.org/en/stable/user-guide/dask.html) or Dask/Spark DataFrames.\n",
    "\n",
    "Xarray-Beam is a lower-level tool. You will be manipulating large datasets piece-by-piece yourself, and you as the developer will be responsible for maintaining Xarray-Beam's internal invariants. This means that to successfully use Xarray-Beam, **you will need to understand how how it represents distributed datasets**. This may sound like a lot of responsibility, but we promise that isn't too bad!\n",
    "```\n",
    "\n",
    "We'll start off with some standard imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import xarray_beam as xbeam\n",
    "import xarray"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/apache_beam/__init__.py:79: UserWarning: This version of Apache Beam has not been sufficiently tested on Python 3.9. You may encounter bugs or missing features.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keys in Xarray-Beam\n",
    "\n",
    "Xarray-Beam is designed around the model that every stage in your Beam pipeline _could_ be stored in a single `xarray.Dataset` object, but is instead represented by a distributed beam `PCollection` of smaller `xarray.Dataset` objects, distributed in two possible ways:\n",
    "\n",
    "- Distinct _variables_ in a Dataset may be separated across multiple records.\n",
    "- Individual arrays can also be split into multiple _chunks_, similar to those used by [dask.array](https://docs.dask.org/en/latest/array.html).\n",
    "\n",
    "To keep track of how individual records could be combined into a larger (virtual) dataset, Xarray-Beam defines a `Key` object. Key objects consist of:\n",
    "\n",
    "1. `offsets`: integer offests for chunks from the origin in an `immutabledict`\n",
    "2. `vars`: The subset of variables included in each chunk, either as a `frozenset`, or as `None` to indicate \"all variables\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Making a `Key` from scratch is simple:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "key = xbeam.Key({'x': 0, 'y': 10}, vars=None)\n",
    "key"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Key(offsets={'x': 0, 'y': 10}, vars=None)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or given an existing `Key`, you can easily modify it with `replace()` or `with_offsets()`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "key.replace(vars={'foo', 'bar'})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Key(offsets={'x': 0, 'y': 10}, vars={'foo', 'bar'})"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "key.with_offsets(x=None, z=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Key(offsets={'y': 10, 'z': 1}, vars=None)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Key` objects don't do very much. They are just simple structs with two attributes, along with various special methods required to use them as `dict` keys or as keys in Beam pipelines. You can find a more examples of manipulating keys in the docstring."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating PCollections"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The standard inputs & outputs for Xarray-Beam are PCollections of tuples of `(xbeam.Key, xarray.Dataset)` pairs. Xarray-Beam provides a bunch of PCollections for typical tasks, but many pipelines will still involve some manual manipulation of `Key` and `Dataset` objects, e.g., with builtin Beam transforms like `beam.Map`.\n",
    "\n",
    "To start off, let's write a helper functions for creating our first collection from scratch:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def create_records():\n",
    "    for offset in [0, 3]:\n",
    "        key = xbeam.Key({'x': offset})\n",
    "        chunk = xarray.Dataset({\n",
    "            'foo': ('x', offset + np.arange(3)),\n",
    "            'bar': ('x', 10 + offset + np.arange(3)),\n",
    "        })\n",
    "        yield key, chunk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In practice, we'd typically feed this into `beam.Create()` to start out pipeline, e.g., `beam.Create(create_records())`. But for now, let's take a look at the two entries:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "inputs = list(create_records())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2\n",
       "      bar      (x) int64 10 11 12),\n",
       " (Key(offsets={'x': 3}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 3 4 5\n",
       "      bar      (x) int64 13 14 15)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{note}\n",
    "If desired, we could have set `vars={'foo', 'bar'}` on each of these `Key` objects instead of `vars=None`. This would be an equally valid representation of the same records, since all of our datasets have the same variables.\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In practice, it's common to have an existing lazy `xarray.Dataset` that needs to be split into a bunch of separate chunks for the inputs to the Beam pipeline. This is where the `DatasetToChunks` transform comes in handy, e.g.,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "ds = xarray.Dataset({'foo': ('x', np.arange(6)), 'bar': ('x', np.arange(10, 16))})\n",
    "\n",
    "# this object could be used as the root of a Beam pipeline\n",
    "xbeam.DatasetToChunks(ds, chunks={'x': 3})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<DatasetToChunks(PTransform) label=[DatasetToChunks] at 0x7fed459ed6a0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: finish this!\n",
    "\n",
    "- Discuss the nuances of feeding in Dask datasets into DatasetToChunks\n",
    "- Discuss options for lazy datasets: xarray's lazy indexing vs dask\n",
    "- ChunksToZarr (including `template`)\n",
    "- Fancy algorithms for rechunking"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('xarray-beam': conda)"
  },
  "interpreter": {
   "hash": "aef148d7ea0dbd1f91630322dd5bc9e24a2135d95f24fe1a9dab9696856be2b9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}