{
  "cells": [
   {
    "cell_type": "markdown",
    "id": "c54a0838",
    "metadata": {},
    "source": [
     "# Reading and writing data"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "ed512674",
    "metadata": {},
    "source": [
     "## Read datasets into chunks"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "30edb09d",
    "metadata": {},
    "source": [
     "There are two main options for loading an `xarray.Dataset` into Xarray-Beam. You can either [create the dataset](data-model.ipynb) from scratch or use the {py:class}`~xarray_beam.DatasetToChunks` transform starting at the root of a Beam pipeline:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 1,
    "id": "427d32c2",
    "metadata": {},
    "outputs": [],
    "source": [
     "import apache_beam as beam\n",
     "import numpy as np\n",
     "import pandas as pd\n",
     "import xarray_beam as xbeam\n",
     "import xarray"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 2,
    "id": "6fe9fabe",
    "metadata": {},
    "outputs": [],
    "source": [
     "ds = xarray.tutorial.load_dataset('air_temperature')"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 3,
    "id": "28eb3b6e",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "application/javascript": [
        "\n",
        "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
        "          var jqueryScript = document.createElement('script');\n",
        "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
        "          jqueryScript.type = 'text/javascript';\n",
        "          jqueryScript.onload = function() {\n",
        "            var datatableScript = document.createElement('script');\n",
        "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
        "            datatableScript.type = 'text/javascript';\n",
        "            datatableScript.onload = function() {\n",
        "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
        "              window.interactive_beam_jquery(document).ready(function($){\n",
        "                \n",
        "              });\n",
        "            }\n",
        "            document.head.appendChild(datatableScript);\n",
        "          };\n",
        "          document.head.appendChild(jqueryScript);\n",
        "        } else {\n",
        "          window.interactive_beam_jquery(document).ready(function($){\n",
        "            \n",
        "          });\n",
        "        }"
       ]
      },
      "metadata": {},
      "output_type": "display_data"
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 0}, vars=None) <class 'xarray.core.dataset.Dataset'>\n",
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 1000}, vars=None) <class 'xarray.core.dataset.Dataset'>\n",
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 2000}, vars=None) <class 'xarray.core.dataset.Dataset'>\n"
      ]
     }
    ],
    "source": [
     "with beam.Pipeline() as p:\n",
     "    p | xbeam.DatasetToChunks(ds, chunks={'time': 1000}) | beam.MapTuple(lambda k, v: print(k, type(v)))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "d7c825b8",
    "metadata": {},
    "source": [
     "Importantly, xarray datasets fed into `DatasetToChunks` **can be lazy**, with data not already loaded eagerly into NumPy arrays. When you feed lazy datasets into `DatasetToChunks`, each individual chunk will be indexed and evaluated separately on Beam workers.\n",
     "\n",
     "This pattern allows for leveraging Xarray's builtin dataset loaders (e.g., `open_dataset()` and `open_zarr()`) for feeding arbitrarily large datasets into Xarray-Beam."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "70f09baa",
    "metadata": {},
    "source": [
     "##  Reading data from Zarr"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "f7229f50",
    "metadata": {},
    "source": [
     "[Zarr](https://zarr.readthedocs.io/) is the preferred file format for reading and writing data with Xarray-Beam, due to its excellent scalability and support inside Xarray.\n",
     "\n",
     "The easiest way to get good performance from Zarr into Xarray-Beam is to use {py:func}`xarray_beam.open_zarr`. This function returns a pair of values:\n",
     "\n",
     "1. A lazily indexed `xarray.Dataset` corresponding to the Zarr store, but not using Dask. This is exactly what you would get from `xarray.open_zarr` with `chunks=None`.\n",
     "2. A dictionary mapping from dimension names to integer chunk sizes. Obtaining this information without using Dask to chunk the array requires looking at Xarray's `encoding` dictionaries or directly inspecting the Zarr store."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 4,
    "id": "10c17dc3",
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 2920, lat: 25, lon: 53)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n",
       "  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n",
       "  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n",
       "Data variables:\n",
       "    air      (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:  COARDS\n",
       "    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n",
       "    platform:     Model\n",
       "    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...\n",
       "    title:        4x daily NMC reanalysis (1948)\n",
       "{'time': 1000, 'lat': 25, 'lon': 53}\n"
      ]
     },
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
       "/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/xarray/core/dataset.py:2060: SerializationWarning: saving variable None with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
       "  return to_zarr(  # type: ignore\n"
      ]
     }
    ],
    "source": [
     "# write data into the distributed Zarr format\n",
     "ds.chunk({'time': 1000}).to_zarr('example-data.zarr', mode='w')\n",
     "\n",
     "# read it using xarray-beam's utilities\n",
     "ds_on_disk, chunks = xbeam.open_zarr('example-data.zarr')\n",
     "print(ds_on_disk)\n",
     "print(chunks)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "83a1833a",
    "metadata": {},
    "source": [
     "Conveniently, this is exactly the information you need for feeding into {py:class}`~xarray_beam.DatasetToChunks` to write an Xarray-Beam pipeline:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 5,
    "id": "7b76ba13",
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 0}, vars=None) <class 'xarray.core.dataset.Dataset'>\n",
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 1000}, vars=None) <class 'xarray.core.dataset.Dataset'>\n",
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 2000}, vars=None) <class 'xarray.core.dataset.Dataset'>\n"
      ]
     }
    ],
    "source": [
     "with beam.Pipeline() as p:\n",
     "    p | xbeam.DatasetToChunks(ds_on_disk, chunks) | beam.MapTuple(lambda k, v: print(k, type(v)))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "169f838c",
    "metadata": {},
    "source": [
     "## Writing data to Zarr"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "5bcf8ad1",
    "metadata": {},
    "source": [
     "{py:class}`~xarray_beam.ChunksToZarr` is Xarray-Beam's API for saving chunks into a Zarr store. \n",
     "\n",
     "For small datasets where you aren't concerned about extra overhead for writing data, you can get started just using it directly:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 6,
    "id": "23b74c9f",
    "metadata": {},
    "outputs": [],
    "source": [
     "with beam.Pipeline() as p:\n",
     "    p | xbeam.DatasetToChunks(ds_on_disk, chunks) | xbeam.ChunksToZarr('example-data-v2.zarr')"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "fd18c764",
    "metadata": {},
    "source": [
     "The resulting Zarr dataset will have array chunks matching the in-memory chunks from your Beam PCollection. If you want different chunks on disk, consider inserting [transforms to rechunk](rechunking.ipynb) before exporting to Zarr.\n",
     "\n",
     "For larger datasets, read on -- you'll want to use a template."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "012d88ee",
    "metadata": {},
    "source": [
     "### Creating templates\n",
     "\n",
     "By default, {py:class}`ChunksToZarr` needs to evaluate and combine the entire distributed dataset in order to determine overall Zarr metadata (e.g., array names, shapes, dtypes and attributes). This is fine for relatively small datasets, but can entail significant additional communication and storage costs for large datasets.\n",
     "\n",
     "The optional `template` argument allows for prespecifying structure of the full on disk dataset in the form of another lazy `xarray.Dataset`. Lazy templates specify the structure of the array data that will be written by the PTransform. Array values that may be written as part of the Beam pipeline are indicated by using lazily computed Dask arrays to store the data.\n",
     "\n",
     "The easiest way to make a template is with {py:func}`xarray_beam.make_template` helper, which transforms a dataset into another dataset where every value is lazy:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 7,
    "id": "bbf65917",
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 2920, lat: 25, lon: 53)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n",
       "  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n",
       "  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n",
       "Data variables:\n",
       "    air      (time, lat, lon) float32 dask.array<chunksize=(2920, 25, 53), meta=np.ndarray>\n",
       "Attributes:\n",
       "    Conventions:  COARDS\n",
       "    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n",
       "    platform:     Model\n",
       "    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...\n",
       "    title:        4x daily NMC reanalysis (1948)\n"
      ]
     }
    ],
    "source": [
     "ds = xarray.open_zarr('example-data.zarr', chunks=None)\n",
     "template = xbeam.make_template(ds)\n",
     "print(template)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "0b21fded",
    "metadata": {},
    "source": [
     "```{tip}\n",
     "Under the covers, {py:func}`~xarray_beam.make_template` has a very simple implementation, equivalent to `xarray.zeros_like(ds.chunk(-1))`.\n",
     "```\n",
     "\n",
     "\"Template\" datasets are not only useful for expressing the desired structures of Zarr stores, but also because every builtin Xarray operation is entirely lazy on Datasets consistenting of Dask arrays. This makes it relatively straightforward to build up a new Dataset with the required variables and dimension, e.g.,"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 8,
    "id": "18eb8a29",
    "metadata": {},
    "outputs": [],
    "source": [
     "# remove the \"time\" dimension, and insert a new \"sample\" dimension\n",
     "new_template = template.isel(time=0, drop=True).expand_dims(sample=np.arange(10))"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 9,
    "id": "eecfc041",
    "metadata": {},
    "outputs": [],
    "source": [
     "# setup a template for spatially regridding along latitude and longitude\n",
     "new_longitudes = np.linspace(0, 100, num=8)\n",
     "new_latitudes = np.linspace(30, 80, num=7)\n",
     "new_template = template.head(lat=7, lon=8).assign_coords(lat=new_latitudes, lon=new_longitudes)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "e3079b5d",
    "metadata": {},
    "source": [
     "### End to end examples\n",
     "\n",
     "If you supply a `template`, you should also supply the `zarr_chunks` argument in order to ensure that the data ends up appropriately chunked in the Zarr store. A complete example of reading and writing data from a Zarr store typically looks something like:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 10,
    "id": "b6bd8cb7",
    "metadata": {},
    "outputs": [],
    "source": [
     "ds_on_disk, chunks = xbeam.open_zarr('example-data.zarr')\n",
     "\n",
     "template = xbeam.make_template(ds_on_disk)\n",
     "\n",
     "with beam.Pipeline() as p:\n",
     "    (\n",
     "        p\n",
     "        | xbeam.DatasetToChunks(ds_on_disk, chunks)\n",
     "        # insert additional transforms here\n",
     "        | xbeam.ChunksToZarr('example-data-v3.zarr', template, chunks)\n",
     "    )"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "d06f806b",
    "metadata": {},
    "source": [
     "If you don't have an existing Dataset to start with, a common pattern is to reuse the same function you'll use to load data for each chunk, e.g.,"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 11,
    "id": "5e161959",
    "metadata": {},
    "outputs": [
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
       "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in '[11]: ConsolidateChunks/GroupByTempKeys'. \n",
       "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in '[11]: ConsolidateChunks/GroupByTempKeys'. \n"
      ]
     }
    ],
    "source": [
     "all_days = pd.date_range('2013-01-01', '2014-01-01', freq='1D')\n",
     "\n",
     "def load_one_example(time: pd.Timestamp) -> tuple[xbeam.Key, xarray.Dataset]:\n",
     "    key = xbeam.Key({'time': (time - all_days[0]).days})\n",
     "    dataset = ds.sel(time=[time])  # replace with your code to create one example\n",
     "    return key, dataset\n",
     "\n",
     "_, example = load_one_example(all_days[0])\n",
     "template = xbeam.make_template(example).squeeze('time', drop=True).expand_dims(time=all_days)\n",
     "zarr_chunks = {'time': 100}  # desired chunking along \"time\", e.g., for more efficient storage in Zarr\n",
     "\n",
     "with beam.Pipeline() as p:\n",
     "    (\n",
     "        p\n",
     "        | beam.Create(all_days)\n",
     "        | beam.Map(load_one_example)\n",
     "        | xbeam.ConsolidateChunks(zarr_chunks)\n",
     "        | xbeam.ChunksToZarr('example-data-v4.zarr', template, zarr_chunks)\n",
     "    )"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "5d5c2e0f",
    "metadata": {},
    "source": [
     "For more examples of how to manipulate templates and read/write data with Zarr, see the end-to-end [ERA5 climatology](https://github.com/google/xarray-beam/blob/main/examples/era5_climatology.py) and [ERA5 rechunk](https://github.com/google/xarray-beam/blob/main/examples/era5_rechunk.py) examples."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "9613b48c",
    "metadata": {},
    "source": [
     "## Tips for custom data loaders"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "d2500f6e",
    "metadata": {},
    "source": [
     "If you use Xarray's file opening utilities instead of {py:class}`xarray_beam.open_zarr`, you need to take some care to get good performance when processing very large numbers of chunks (hundreds of thousands).\n",
     "\n",
     "The main tip is to set `chunks=None` when opening datasets and then _explicitly_ provide chunks in `DatasetToChunks` -- exactly the pattern facilitated by `xarray_beam.open_zarr`.\n",
     "\n",
     "`chunks=None` tells Xarray to use its builtin lazy indexing machinery, instead of using Dask. This is advantageous because datasets using Xarray's lazy indexing are serialized much more compactly (via [pickle](https://docs.python.org/3/library/pickle.html)) when passed into Beam transforms."
    ]
   },
   {
    "cell_type": "markdown",
    "id": "7d3ec100",
    "metadata": {},
    "source": [
     "Alternatively, you can pass in lazy datasets [using dask](http://xarray.pydata.org/en/stable/user-guide/dask.html). In this case, you don't need to explicitly supply `chunks` to `DatasetToChunks`:"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 12,
    "id": "d3f4f0a0",
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 0}, vars=None) <class 'xarray.core.dataset.Dataset'>\n",
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 1000}, vars=None) <class 'xarray.core.dataset.Dataset'>\n",
       "Key(offsets={'lat': 0, 'lon': 0, 'time': 2000}, vars=None) <class 'xarray.core.dataset.Dataset'>\n"
      ]
     }
    ],
    "source": [
     "on_disk = xarray.open_zarr('example-data.zarr', chunks={'time': 1000})\n",
     "\n",
     "with beam.Pipeline() as p:\n",
     "    p | xbeam.DatasetToChunks(on_disk) | beam.MapTuple(lambda k, v: print(k, type(v)))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "1a9a5810",
    "metadata": {},
    "source": [
     "Dask's lazy evaluation system is much more general than Xarray's lazy indexing, so as long as resulting dataset can be independently evaluated in each chunk using Dask can be a very convenient way to setup computation for Xarray-Beam.\n",
     "\n",
     "Unfortunately, it doesn't scale as well. In particular, the overhead of pickling large Dask graphs for passing to Beam workers can be prohibitive for large (multiple TB) datasets with millions of chunks. There are [plans to eventually fix this in Dask](https://github.com/dask/distributed/issues/5581), but in the meantime, prefer the pattern of using Dask arrays with single chunks (e.g., as created by `make_template`), with separate explicit specification of array chunks."
    ]
   }
  ],
  "metadata": {
   "celltoolbar": "Tags",
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.9.13"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }